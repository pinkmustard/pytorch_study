{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do it 자연어 처리 3장 숫자 세계로 떠난 자연어\n",
    "## 3.4 트랜스포머에 적용된 기술들\n",
    "### 피드포워드 뉴럴 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피드포워드 뉴럴 네트워크 계산 예시\n",
    "import torch\n",
    "x = torch.tensor([2, 1])\n",
    "w1 = torch.tensor([[3,2,-4], [2,-3,1]])\n",
    "b1 = 1\n",
    "w2 = torch.tensor([[-1, 1], [1,2], [3,1]])\n",
    "b2 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_preact = torch.matmul(x, w1) + b1 # 행렬곱 + 바이어스 수행\n",
    "h = torch.nn.functional.relu(h_preact) # 활성함수 거침\n",
    "y = torch.matmul(h, w2) + b2 # 두번째 은닉층 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9,  2, -6])\n",
      "tensor([9, 2, 0])\n",
      "tensor([-8, 12])\n"
     ]
    }
   ],
   "source": [
    "print(h_preact)\n",
    "print(h)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차연결\n",
    "- 모델이 다양한 관점에서 블록 계산을 수행\n",
    "- 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 쉽게 하는 효과를 거둘 수 있음\n",
    "\n",
    "### 레이어 정규화\n",
    "- 학습이 안정되고 그 속도가 빨라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2247,  0.0000,  1.2247],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<NativeLayerNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 레이어 정규화 예시\n",
    "import torch\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n",
    "m = torch.nn.LayerNorm(input.shape[-1]) # 레이어 정규화 코드\n",
    "output = m(input)\n",
    "print(output)\n",
    "print(m.weight)\n",
    "print(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드롭아웃\n",
    "- 과적합 방지\n",
    "- 일부 노드를 확률적으로 0으로 대치하여 계산에서 제외\n",
    "- 10%가 일반적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3482,  1.7226,  0.9384,  0.2712,  2.2820,  0.9701,  0.3257,  1.5054,\n",
      "         -1.4174, -0.7939]])\n",
      "tensor([[ 0.4352,  2.1533,  1.1730,  0.0000,  2.8525,  0.0000,  0.4071,  1.8818,\n",
      "         -1.7717, -0.9924]])\n"
     ]
    }
   ],
   "source": [
    "# 드롭 아웃 예시\n",
    "import torch\n",
    "m = torch.nn.Dropout(p=0.2) # 안정적인 학습을 위해 각 요솟값에 1/(1-p)를 곱하기도 함\n",
    "input = torch.randn(1,10)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아담 옵티마이저\n",
    "- 딥러닝 모델 학습: 출력-정답 사이 오차 최소화, 파라미터 업데이트 과정\n",
    "- 손실(loss): 오차\n",
    "- 기울기(gradient): 오차를 최소화하는 방향\n",
    "- 최적화(optimization): 오차를 최소화하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아담 옵티마이저 예시\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=model.learning_rate) \n",
    "# lr을 정해주면 아담이 model.parameters(최적화 대상 파라미터들)에 방향과 보폭을 정해줌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
