{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do it 자연어 처리 3장 숫자 세계로 떠난 자연어\n",
    "## 3.4 트랜스포머에 적용된 기술들\n",
    "### 피드포워드 뉴럴 네트워크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피드포워드 뉴럴 네트워크 계산 예시\n",
    "import torch\n",
    "x = torch.tensor([2, 1])\n",
    "w1 = torch.tensor([[3,2,-4], [2,-3,1]])\n",
    "b1 = 1\n",
    "w2 = torch.tensor([[-1, 1], [1,2], [3,1]])\n",
    "b2 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_preact = torch.matmul(x, w1) + b1 # 행렬곱 + 바이어스 수행\n",
    "h = torch.nn.functional.relu(h_preact) # 활성함수 거침\n",
    "y = torch.matmul(h, w2) + b2 # 두번째 은닉층 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9,  2, -6])\n",
      "tensor([9, 2, 0])\n",
      "tensor([-8, 12])\n"
     ]
    }
   ],
   "source": [
    "print(h_preact)\n",
    "print(h)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잔차연결\n",
    "- 모델이 다양한 관점에서 블록 계산을 수행\n",
    "- 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 쉽게 하는 효과를 거둘 수 있음\n",
    "\n",
    "### 레이어 정규화\n",
    "- 학습이 안정되고 그 속도가 빨라짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2247,  0.0000,  1.2247],\n",
      "        [ 0.0000,  0.0000,  0.0000]], grad_fn=<NativeLayerNormBackward0>)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 레이어 정규화 예시\n",
    "import torch\n",
    "input = torch.tensor([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]])\n",
    "m = torch.nn.LayerNorm(input.shape[-1]) # 레이어 정규화 코드\n",
    "output = m(input)\n",
    "print(output)\n",
    "print(m.weight)\n",
    "print(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드롭아웃\n",
    "- 과적합 방지\n",
    "- 일부 노드를 확률적으로 0으로 대치하여 계산에서 제외\n",
    "- 10%가 일반적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3482,  1.7226,  0.9384,  0.2712,  2.2820,  0.9701,  0.3257,  1.5054,\n",
      "         -1.4174, -0.7939]])\n",
      "tensor([[ 0.4352,  2.1533,  1.1730,  0.0000,  2.8525,  0.0000,  0.4071,  1.8818,\n",
      "         -1.7717, -0.9924]])\n"
     ]
    }
   ],
   "source": [
    "# 드롭 아웃 예시\n",
    "import torch\n",
    "m = torch.nn.Dropout(p=0.2) # 안정적인 학습을 위해 각 요솟값에 1/(1-p)를 곱하기도 함\n",
    "input = torch.randn(1,10)\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아담 옵티마이저\n",
    "- 딥러닝 모델 학습: 출력-정답 사이 오차 최소화, 파라미터 업데이트 과정\n",
    "- 손실(loss): 오차\n",
    "- 기울기(gradient): 오차를 최소화하는 방향\n",
    "- 최적화(optimization): 오차를 최소화하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아담 옵티마이저 예시\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(model.parameters(), lr=model.learning_rate) \n",
    "# lr을 정해주면 아담이 model.parameters(최적화 대상 파라미터들)에 방향과 보폭을 정해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.5 Bert와 GPT 비교\n",
    "### GPT\n",
    "- 언어모델\n",
    "- 일방향성\n",
    "- 문장 생성에 적합\n",
    "- 디코더만 취해 사용\n",
    "### Bert\n",
    "- 마스크 언어 모델\n",
    "- 양방향성\n",
    "- 문장의 의미 추출하는데 적합\n",
    "- 인코더만 취해 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.6 단어/문장을 벡터로 변환\n",
    "### 파인튜닝\n",
    "- 프리트레인 모델과 그 위 작은 모듈을 포함한 전체 모델을 다운스트림 데이터로 업데이트 하는 과정\n",
    "\n",
    "### 문장 벡터 활용 : 문서 분류\n",
    "- pooler_out 벡터(최종출력)를 만들어 하나로 응집 시킴\n",
    "- 긍정, 부정, 중립 등 확률을 배정하고 정답과 가까워지게 함 -> 파인튜닝\n",
    "\n",
    "### 단어 벡터 활용 : 개체명 인식\n",
    "- 마지막 블록이 모든 단어 벡터를 활용\n",
    "- 각 개체명 범주가 될 확률을 만들고 정답과 가까워지게 함\n",
    "\n",
    "### 문장을 벡터로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.txt: 100%|██████████| 250k/250k [00:00<00:00, 2.18MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<00:00, 48.6kB/s]\n",
      "config.json: 100%|██████████| 619/619 [00:00<00:00, 1.69MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 선언\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:07<00:00, 56.2MB/s] \n",
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 모델 선언\n",
    "from transformers import BertConfig, BertModel\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    \"beomi/kcbert-base\"\n",
    ")\n",
    "model = BertModel.from_pretrained(\n",
    "    \"beomi/kcbert-base\",\n",
    "    config = pretrained_model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 300,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.28.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값 만들기\n",
    "sentences = ['안녕하세요', '하이!']\n",
    "features = tokenizer(\n",
    "    sentences,\n",
    "    max_length=10,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 19017, 8482, 3, 0, 0, 0, 0, 0, 0], [2, 15830, 5, 3, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features\n",
    "# 2, 3은 스페셜 토큰 CLS, SEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처를 토치 텐서로 변환\n",
    "features = {k: torch.tensor(v) for k, v in features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 계산하기\n",
    "outputs = model(**features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6969, -0.8248,  1.7512,  ..., -0.3732,  0.7399,  1.1907],\n",
       "         [-1.4803, -0.4398,  0.9444,  ..., -0.7405, -0.0211,  1.3064],\n",
       "         [-1.4299, -0.5033, -0.2069,  ...,  0.1285, -0.2611,  1.6057],\n",
       "         ...,\n",
       "         [-1.4406,  0.3431,  1.4043,  ..., -0.0565,  0.8450, -0.2170],\n",
       "         [-1.3625, -0.2404,  1.1757,  ...,  0.8876, -0.1054,  0.0734],\n",
       "         [-1.4244,  0.1518,  1.2920,  ...,  0.0245,  0.7572,  0.0080]],\n",
       "\n",
       "        [[ 0.9371, -1.4749,  1.7351,  ..., -0.3426,  0.8050,  0.4031],\n",
       "         [ 1.6095, -1.7269,  2.7936,  ...,  0.3100, -0.4787, -1.2491],\n",
       "         [ 0.4861, -0.4569,  0.5712,  ..., -0.1769,  1.1253, -0.2756],\n",
       "         ...,\n",
       "         [ 1.2362, -0.6181,  2.0906,  ...,  1.3677,  0.8132, -0.2742],\n",
       "         [ 0.5409, -0.9652,  1.6237,  ...,  1.2395,  0.9185,  0.1782],\n",
       "         [ 1.9001, -0.5859,  3.0156,  ...,  1.4967,  0.1924, -0.4448]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bert 모델의 여러 출력 결과를 한데 묶은 것\n",
    "# 안녕하세요, 하이!의 입력 토큰 각각에 해당하는 Bert의 마지막 레이어 출력 벡터들\n",
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 2개에 속한 각각의 토큰(길이 10)이 768차원짜리의 벡터로 변환\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1594,  0.0547,  0.1101,  ...,  0.2684,  0.1596, -0.9828],\n",
       "        [-0.9221,  0.2969, -0.0110,  ...,  0.4291,  0.0311, -0.9955]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 수준 벡터\n",
    "outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 2개에 속한 각각의 토큰이 768차원짜리의 벡터로 변환\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 수준 벡터 임베딩: outputs.last_hidden_state\n",
    "- 문장 수준 벡터 임베딩: outputs.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태스크 모듈 만들기\n",
    "- 파인 튜닝을 수행하기 위해 태스크 수행을 위한 작은 모듈을 추가해야함\n",
    "- 다운 스트림 태스크 별로 조금씩 달라짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
